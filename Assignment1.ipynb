{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 1\n",
    "\n",
    "- Do not submit your answers as images where text or an equation is expected. You might be evaluated with a zero.\n",
    "- Use mathlatex (latex notations) to type math equations\n",
    "- If at all you feel you need to add some diagram or illustration, use relative path to add them as image and make sure you include them in the zipped archive that you will be submitting in the moodle\n",
    "- Name your notebook and the zip as < rollno >_A1.zip. For example if you are roll number 13CS60R12 then submit the zip as 13CS60R12_A1.zip\n",
    "\n",
    "- The marks for the individual questions will be decided later\n",
    "- Double click on the cells where it is written \"Ans. Write your answer here.\". Markdown syntax needs to be followed while writing the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. \n",
    "We are given a dataset $(X,Y)$.  Which among the following classifiers will contain sufficient information that allows the calculation of joint probability of the features and the label in the dataset? Justify your answer for each of the classifer.\n",
    "If $X = (X_1,X_2,X_3,X_4)$ then you need to calculate $P(X_1,X_2,X_3,X_4,Y)$.\n",
    "\n",
    " - Linear Regression\n",
    " - Logisitc Regression\n",
    " - Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Linear Regresion: NO, we cannot compute $P(X)$.\n",
    "Logistic Regression: NO, we cannot compute $P(X)$.\n",
    "Gaussian Naive Bayes:  Yes, we can estimate $P(X1, X2, X3, Y ) = P(Y )P(X1|Y )P(X2|Y )P(X3|Y )$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2a. \n",
    "For two discrete-valued distributions $P(X), Q(X)$, K-L Divergence is defined as\n",
    " \n",
    "$$ KL(P||Q) = \\sum_x P(x)log\\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "where P(x) > 0. \n",
    "\n",
    "Prove the following $$ \\forall P,Q~~ KL(P||Q) \\geq 0 $$ and\n",
    "$$ KL(P||Q) ~ = ~ 0 ~ iff ~ P ~ = ~ Q$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The above results is known as Gibb's inequality. Suppose that $$P=\\{p_{1},\\ldots ,p_{n}\\}$$ is a probability distribution. then for any other probability distribution $$Q=\\{q_{1},\\ldots ,q_{n}\\}$$. the above inequality holds with equality when P=Q.\n",
    "Proof : \n",
    "It is sufficient to prove the statement using the natural logarithm (ln). Note that the natural logarithm satisfies\n",
    "\n",
    " $$ \\ln x \\leq x-1 $$\n",
    "for all x > 0 with equality if and only if x=1.\n",
    "\n",
    "Let $I$ denote the set of all $i$ for which probabillity is non zero. Then\n",
    "\n",
    "$$\\sum _{i\\in I}p_{i}\\ln {\\frac {q_{i}}{p_{i}} \\geq -\\sum _{i\\in I}p_{i}\\left({\\frac {q_{i}}{p_{i}}}-1\\right)} -\\sum _{{i\\in I}}p_{i}\\ln {\\frac  {q_{i}}{p_{i}}}\\geq -\\sum _{{i\\in I}}p_{i}\\left({\\frac  {q_{i}}{p_{i}}}-1\\right)$$\n",
    "$$ =-\\sum _{{i\\in I}}q_{i}+\\sum _{{i\\in I}}p_{i}$$\n",
    "$$\\geq 0.$$\n",
    "So\n",
    "\n",
    "$$-\\sum _{i\\in I}p_{i}\\ln q_{i}\\geq -\\sum _{i\\in I}p_{i}\\ln p_{i} -\\sum _{{i\\in I}}p_{i}\\ln q_{i}\\geq -\\sum _{{i\\in I}}p_{i}\\ln p_{i}$$\n",
    "and then trivially\n",
    "\n",
    "$$-\\sum _{i=1}^{n}p_{i}\\ln q_{i}\\geq -\\sum _{{i=1}}^{n}p_{i}\\ln p_{i}$$\n",
    "since the right hand side does not grow, but the left hand side may grow or may stay the same.\n",
    "\n",
    "For equality to hold, we require:\n",
    "$${\\frac {q_{i}}{p_{i}}}=1~~for~all~i\\in I $$ that the approximation $$ \\ln {\\frac  {q_{i}}{p_{i}}}={\\frac  {q_{i}}{p_{i}}}-1 ~~$$is exact.\n",
    "$$ \\sum _{{i\\in I}}q_{i}=1 $$ so that equality continues to hold between the third and fourth lines of the proof.\n",
    "This can happen if and only if\n",
    "$$ p_{i}=q_{i}~~for ~~i = 1, ..., n.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Q2b.\n",
    "The KL-Divergence between two conditional distributions $P(X|Y),Q(X|Y)$ is\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\bigg( \\sum_x P(x|y)log\\frac{P(x|y)}{Q(x|y)} \\bigg)$$\n",
    "\n",
    "\n",
    "Prove the following chain rule for KL Divergence:\n",
    "$$ KL(P(X|Y)||Q(X|Y) = KL(P(X)|Q(X)) + KL(P(Y|X)||Q(Y|X)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\bigg( \\sum_x P(x|y)log\\frac{P(x|y)}{Q(x|y)} \\bigg)$$\n",
    "$$ = ~ \\sum_y ~\\sum_x~P(x)P(y|x) log \\frac{P(x)P(y|x)}{Q(x)Q(y|x)}$$\n",
    "$$ = ~ \\sum_y ~\\sum_x~P(x)P(y|x) log \\frac{P(x)}{Q(x)}~+~ \\sum_y ~\\sum_x~P(x)P(y|x) log \\frac{P(y|x)}{Q(y|x)}$$\n",
    "$$ = ~ \\sum_x ~P(x) log \\frac{P(x)}{Q(x)}~\\sum_y ~P(y|x)~+~\\sum_x~P(x)~\\sum_y~P(y|x) log \\frac{P(y|x)}{Q(y|x)}$$\n",
    "$$ = ~KL(P(x)||Q(x))~+~KL(P(y|x)||Q(y|x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "What is the role of the activation function in a neural network? What would happen if you just used the identify function as an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. For instance,the logistic regression classifier has a non-linear activation function, but the weight coefficients of this model are essentially a linear combination, which is why logistic regression is a \"generalized\" linear model. Now, the role of the activation function in a neural network is to produce a non-linear decision boundary via non-linear combinations of the weighted inputs.\n",
    "Thus sigmoid functions where introduced because they are differentiable, in fact all modern activation functions are continuous and differentiable apart from the rectified units at zero but this is not a problem because sub-gradients can still be used.\n",
    "\n",
    "Thus the activation function is a decision function and one needs non-linear decision functions that is why all activation functions have some non-linearity in them. \n",
    "\n",
    "The purpose of the activation function is to introduce non-linearity into the network in turn, this allows you to model a response variable (aka target variable, class label, or score) that varies non-linearly with its explanatory variables\n",
    "\n",
    "Non-linear means that the output cannot be reproduced from a linear combination of the inputs (which is not the same as output that renders to a straight line--the word for this is affine).\n",
    "\n",
    "Without a non-linear activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "Assume a friend of yours recently got diogonised for a rare disease and it is given that the testing methods for this disease are correct 99 percent of the time. You did some googling and found that the chances of the disease to occur randomly in the general population is only one of every 10,000 people. What are the chances that your friend actually have the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Probability of actually having disease is $$\\frac{1}{10000}~*~\\frac{99}{100} ~+~ \\frac{9999}{10000}~*~\\frac{1}{100}$$\n",
    "$$= ~\\frac{10098}{1000000}$$\n",
    "$$= ~0.010098$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Q5. \n",
    "How exactly is the training of structured perceptron different from that of a perceptron? Explain how we can solve argmax problem for sequences in the context of a structured perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The structured perceptron is an extension of the standard perceptron to\n",
    "structured prediction. Importantly, it is only applicable to the problem of 0/1 loss over $Y$: that is, $l(x, y, yˆ) = 1(y\\neq yˆ)$. As such, it only solves decomposable structured prediction problems (0/1 loss is trivially invariant under permutations). Like all the algorithms we consider, the structured perceptron will be parameterized by weight vector $w$. The structured perceptron makes one significant assumption: that\n",
    "$argmax~equation$ can be solved efficiently.\n",
    "Based on the argmax assumption, the structured perceptron constructs the perceptron in nearly an identical manner as for the binary case. While looping through the training data, whenever the predicted $y_n$ for $x_n$ differs from $y_n$, we update the weights according\n",
    "to Equation.\n",
    "$$w ~← ~w~+~Φ(x_n, y_n)~−~Φ(x_n, yˆ_n)$$\n",
    "This weight update serves to bring the vector closer to the true output and further\n",
    "from the incorrect output. As in the standard perceptron, this often leads to a learned\n",
    "model that generalizes poorly. As before, one solution to this problem is weight averaging.\n",
    "This behaves identically to the averaged binary perceptron and the full training algorithm.\n",
    "The behavior of the structured perceptron and the standard perceptron are virtually\n",
    "identically. The major changes are as follow. First, there is no bias $b$. For structured\n",
    "problems, a bias is irrelevant: it will increase the score of all hypothetical outputs by the\n",
    "same amount. The next major difference is : the best scoring output $yˆ_n$ for\n",
    "the input $x_n$ is computed using the arg max. After checking for an error, the weights are\n",
    "updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "## Q6. \n",
    "We already know differentiation in the context of univariate real valued functions (input $\\in \\mathbb{R}$, output $\\in \\mathbb{R}$). For e.g $\\frac{d}{dx}(x^2 +x) = 2x + 1$.\n",
    "\n",
    "Now we define differentiation in the context of matrices and vectors. Consider a function $f(\\mathbf{x}) = \\mathbf{y}$, where $\\mathbf{x} = (x_1, x_2, \\dots, x_n)^T \\in \\mathbb{R}^n$ and $\\mathbf{y} = (y_1, y_2, \\dots, y_m)^T \\in \\mathbb{R}^m$ are vectors. We define the derivative of $f(\\mathbf{x})$ wrt $\\mathbf{x}$ as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x}) = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\[2mm]\n",
    "\t\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}\\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\t\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\\\\\n",
    "\t\\end{bmatrix}\n",
    "\\text{ or } \\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_{ij} = \\frac{\\partial y_i}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{x}$ is a scalar (denote by $x$) then we define the derivative as a vector of elementwise derivatives given by\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial x} f(x)\\right]_i = \\frac{\\partial y_i}{\\partial x}\n",
    "$$\n",
    "Similarly, if $\\mathbf{y}$ is a scalar (denote by $y$) then we define the derivative as,\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_i = \\frac{\\partial y}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{y}$ is a scalar (denote by $y$) and $\\mathbf{x}$ is a matrix (denote by $X$), then we define the derivative as a matrix given by\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial X} f(X)\\right]_{ij} = \\frac{\\partial y}{\\partial X_{ij}}\n",
    "$$\n",
    "\n",
    "Given that $A, X\\in \\mathbb{R}^{a\\times b}$ and $v, x\\in \\mathbb{R}^{b}$, show the following:\n",
    " - $\\frac{\\partial}{\\partial x} v^T x = \\frac{\\partial}{\\partial x} x^T v = v$\n",
    " - $\\frac{\\partial}{\\partial x} Ax = A$\n",
    " - $\\frac{\\partial}{\\partial x} x^TAx = Ax + A^Tx$\n",
    "\n",
    "Using the above results, show the following result (which is actually the solution to least squares linear regression)\n",
    "$$\n",
    "    \\underset{w}{\\arg \\min} \\| Xw-Y\\|_2^2 = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "(Hint: $\\|v\\|_2^2 = v^Tv$. Write the above norm in this form, differentiate and equate to zero.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Q7a \n",
    "You are given a Neural Network model which claims to detect between Huskies and Wolves. You are also shown the predictions of the model on 10 held-out images. \n",
    "\n",
    "The results show amongs 10 images (5 each from both the classes), it mis-predicts the 2 cases (1 each from each of the class - 6th and 9th images) of the 10 images.\n",
    "\n",
    "\n",
    "- 1) How much do you trust the model? Give a subject evlaution of the model\n",
    "- 2) What do you think is the system learning?\n",
    "\n",
    "\n",
    "This particular question does not look for the exact answer and rather this question wants to test your thinking and reasoning capacity. So try to come up with multiple possible explanations. <i>The subsequent question will show what exactly was the neural network learning, and hence it is implied that we expect different answers from what is given below. So attempt the next question only after finishing this question</i> \n",
    "\n",
    "![Wolf or Huskies](images/7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. I trust this model 80% as the acccuracy within given samples is 80%. According to given data the model seems to be unbiased about the predictions and seems to be over sensitive to photo exposure.\n",
    "I think the model is learning the difference between the the facial hair of the husky and wolf, also it might be focusing on the exposure of the photo as both incorrect assumptions belongs to either over-exposed or under-exposed photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 7b) \n",
    "Given below are the previously shown images along with its corresponding reconstructed portions obtained from the neural network.\n",
    "\n",
    "The non-gray parts on the reconstructed images are the parts of the image that the neural network thinks are the most important in making the predictions. With the new evidence (assuming you answered previous question without looking into this), please reanswer the above question\n",
    "- 1) How much do you trust the model? Give a subject evlaution of the model\n",
    "- 2) What do you think is the system learning?\n",
    "\n",
    "\n",
    "![Double click and remove the exclamation mark inside the parenthesis to see the image](images/7b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. I trust this model 80% as the acccuracy within given samples is 80%. According to given data the model seems to be unbiased about the predictions and seems to be over sensitive to the background texture and colours so the proper edge detection is missing from this model.\n",
    "I think the model is learning on the basis of contour detection and focusing on various light exposed parts rather than first doing the edge detection and then contour detection within the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment was originally conducted as part of the work. [\"Why Should I Trust You?\"](http://www.arxiv.org/abs/1602.04938): Explaining the Predictions of Any Classifier. \n",
    "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. In: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining \n",
    "    \n",
    "[Marco](https://homes.cs.washington.edu/~marcotcr/) was kindful enough to share the images used in the experiment"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
